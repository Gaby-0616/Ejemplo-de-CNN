{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importan las bibliotecas:\n",
    "\n",
    "1. torch: proporciona el núcleo para construir y entrenar redes neuronales en PyTorch.\n",
    "2. torch.nn: contiene los módulos de construcción de redes neuronales.\n",
    "3. torch.optim: incluye optimizadores para entrenar el modelo.\n",
    "4. torchvision.datasets y transforms: para cargar y transformar datos (en este caso, el conjunto MNIST).\n",
    "5. DataLoader: para gestionar los datos de entrenamiento y prueba en lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas necesarias\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preprocesamiento de los datos:\n",
    "   * transforms.Compose aplica transformaciones secuenciales:\n",
    "   * transforms.ToTensor(): convierte las imágenes a tensores.\n",
    "   * transforms.Normalize((0.5,), (0.5,)): normaliza los datos a un rango entre -1 y 1.\n",
    "2. Carga del conjunto de datos MNIST:\n",
    "   * datasets.MNIST: descarga el conjunto de datos MNIST.\n",
    "   * train=True y train=False definen los datos de entrenamiento y prueba.\n",
    "3. DataLoader:\n",
    "   * train_loader y test_loader gestionan los datos en lotes de tamaño 64.\n",
    "   * shuffle=True en train_loader mezcla los datos en cada época."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:01<00:00, 9.39MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 599kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 5.00MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocesamiento y carga del conjunto de datos MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Esta clase define la arquitectura de la CNN:\n",
    "\n",
    "    * conv1: primera capa convolucional con 32 filtros de 3x3, que toma imágenes con 1 canal.\n",
    "    * conv2: segunda capa convolucional con 64 filtros de 3x3.\n",
    "    * pool: capa de Max Pooling de tamaño 2x2, reduce la dimensionalidad.\n",
    "    * fc1: primera capa densa con 128 neuronas.\n",
    "    * fc2: capa de salida con 10 neuronas (una para cada clase en MNIST, dígitos del 0 al 9).\n",
    "    \n",
    "2. En el método forward se define el flujo de datos:\n",
    "\n",
    "    * Las imágenes pasan por conv1, se aplica la activación ReLU y Max Pooling.\n",
    "    * Luego pasan por conv2, se aplica ReLU y Max Pooling.\n",
    "    * Aplanamiento: x.view(-1, 64 * 7 * 7) convierte la salida de las capas convolucionales en un vector.\n",
    "    * fc1 y fc2 procesan el vector a través de capas densas, obteniendo la salida final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la arquitectura de la CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)  # Aplanar la salida\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se inicializan:\n",
    "\n",
    "* Modelo: instancia de la clase CNN.\n",
    "* Función de pérdida: CrossEntropyLoss, adecuada para clasificación multiclase.\n",
    "* Optimizador: Adam ajusta los pesos del modelo usando una tasa de aprendizaje de 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el modelo, la función de pérdida y el optimizador\n",
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Configuración del entrenamiento:\n",
    "    * El modelo se entrena por 5 épocas.\n",
    "    * running_loss acumula la pérdida en cada lote para calcular la pérdida promedio.\n",
    "2. Entrenamiento en cada lote:\n",
    "   * optimizer.zero_grad(): reinicia los gradientes antes de cada actualización.\n",
    "   * Propagación hacia adelante: model(images) calcula las predicciones.\n",
    "3. Cálculo de la pérdida: criterion(outputs, labels) evalúa qué tan lejos están las predicciones de las etiquetas.\n",
    "   * Propagación hacia atrás: loss.backward() calcula gradientes.\n",
    "   * Actualización de pesos: optimizer.step() ajusta los pesos del modelo.\n",
    "   * Impresión de la pérdida promedio: muestra la pérdida promedio por época."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1, Pérdida de entrenamiento: 0.1600990141394423\n",
      "Época 2, Pérdida de entrenamiento: 0.04564293741044014\n",
      "Época 3, Pérdida de entrenamiento: 0.029993323549318728\n",
      "Época 4, Pérdida de entrenamiento: 0.02342565172795094\n",
      "Época 5, Pérdida de entrenamiento: 0.017415261241508516\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Época {epoch+1}, Pérdida de entrenamiento: {running_loss/len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluación del modelo:\n",
    "\n",
    "1. torch.no_grad(): desactiva el cálculo de gradientes, optimizando la evaluación.\n",
    "2. Propagación hacia adelante en cada lote: model(images) obtiene las predicciones.\n",
    "3. torch.max(outputs.data, 1): obtiene la clase predicha con mayor probabilidad.\n",
    "4. Calculo de precisión: calcula cuántas predicciones fueron correctas y divide entre el total para obtener la precisión en el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión en el conjunto de prueba: 0.9914\n"
     ]
    }
   ],
   "source": [
    "# Evaluación del modelo\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(\"Precisión en el conjunto de prueba:\", correct / total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
